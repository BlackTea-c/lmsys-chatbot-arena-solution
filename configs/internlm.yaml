model_name: internlm/internlm2_5-7b-chat
exp_name: llm_surround

lr: 1.0e-4
epochs: 1
weight_decay: 0.01

lora_r: 64
lora_alpha: 16
target_modules:
  - w1
  - w2
  - w3
  - wqkv
  - wo

training:
  batch_size: 4
  accum: 2
  max_length: 1800

validation:
  batch_size: 2
  accum: 1
  max_length: 8192